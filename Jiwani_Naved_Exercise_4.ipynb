{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Naved555/Spring_5731/blob/main/Jiwani_Naved_Exercise_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 4**\n",
        "\n",
        "**This exercise will provide a valuable learning experience in working with text data and extracting features using various topic modeling algorithms. Key concepts such as Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA), lda2vec, and BERTopic.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M68ekMa2sxVJ",
        "outputId": "9942ef4b-caf9-4770-8e2c-40802b6698fa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ],
      "metadata": {
        "id": "TU-pLW33lpcS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "\n",
        "**Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "468a4ecf-941e-4f84-c4f6-c58db191f0df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    0                      1                    2  \\\n",
            "0  ID                   Name             Birthday   \n",
            "1   0           Kay Aiko Abe  1927-05-09T00:00:00   \n",
            "2   1                Art Abe  1921-06-12T00:00:00   \n",
            "3   2  Sharon Tanagi Aburano  1925-10-31T00:00:00   \n",
            "4   3        Toshiko Aiboshi  1928-04-08T00:00:00   \n",
            "\n",
            "                                                   3  \\\n",
            "0                                                Bio   \n",
            "1  Nisei female. Born May 9, 1927, in Selleck, Wa...   \n",
            "2  Nisei male. Born June 12, 1921, in Seattle, Wa...   \n",
            "3  Nisei female. Born October 31, 1925, in Seattl...   \n",
            "4  Nisei female. Born July 8, 1928, in Boyle Heig...   \n",
            "\n",
            "                                                   4  \n",
            "0                                        Cleaned_Bio  \n",
            "1  nisei femal born may 9 1927 selleck washington...  \n",
            "2  nisei male born june 12 1921 seattl washington...  \n",
            "3  nisei femal born octob 31 1925 seattl washingt...  \n",
            "4  nisei femal born juli 8 1928 boyl height calif...  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "import csv\n",
        "import json\n",
        "import nltk\n",
        "import pandas as pd\n",
        "nltk.download('stopwords')\n",
        "\n",
        "with open('/root/cleaned_data.csv', 'r') as f:\n",
        "    data = list(csv.reader(f))\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0t1_bVYv9IE",
        "outputId": "01668ed9-d387-47d7-9653-1d4e793d42e3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/2.6 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m2.3/2.6 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.11.4)\n",
            "Collecting pandas>=2.0.0 (from pyLDAvis)\n",
            "  Downloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.3)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.9.0)\n",
            "Collecting funcy (from pyLDAvis)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (4.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2023.4)\n",
            "Collecting tzdata>=2022.7 (from pandas>=2.0.0->pyLDAvis)\n",
            "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.4.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyLDAvis) (6.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (2.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
            "Installing collected packages: funcy, tzdata, pandas, pyLDAvis\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed funcy-2.0 pandas-2.2.1 pyLDAvis-3.4.1 tzdata-2024.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas==1.5.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "kKTBieXov9WG",
        "outputId": "7a49fd1d-0338-417d-bc62-6ef672c40d2b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas==1.5.3\n",
            "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.16.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.1\n",
            "    Uninstalling pandas-2.2.1:\n",
            "      Successfully uninstalled pandas-2.2.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-1.5.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              },
              "id": "b98089bc227c43879459af25d6f70fbe"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "from gensim.models import LdaMulticore\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import CoherenceModel\n",
        "import re\n",
        "from gensim.utils import simple_preprocess\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pyLDAvis\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Read data into nav DataFrame\n",
        "nav = pd.read_csv('/root/cleaned_data.csv')\n",
        "print(nav.columns)\n",
        "\n",
        "# Preprocess text data\n",
        "nav['processed_text'] = nav['Name'].map(lambda x: re.sub('[,!?]', '', x))\n",
        "nav['processed_text'] = nav['processed_text'].map(lambda x: x.lower())\n",
        "\n",
        "# Tokenize and remove stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    return [word for word in simple_preprocess(text) if word not in stop_words]\n",
        "\n",
        "nav['tokenized_text'] = nav['processed_text'].map(tokenize)\n",
        "\n",
        "# Create a dictionary and corpus\n",
        "id2word = Dictionary(nav['tokenized_text'])\n",
        "corpus = [id2word.doc2bow(text) for text in nav['tokenized_text']]\n",
        "\n",
        "# Determine the optimal number of topics using coherence score\n",
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=1):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=num_topics, workers=1)  # Set workers to 1\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "\n",
        "    return model_list, coherence_values\n",
        "\n",
        "# Compute coherence scores\n",
        "start_topic = 2  # Specify the starting number of topics\n",
        "limit_topic = 20  # Specify the upper bound for the number of topics\n",
        "step_topic = 1  # Specify step size\n",
        "model_list, coherence_values = compute_coherence_values(dictionary=id2word,\n",
        "                                                        corpus=corpus,\n",
        "                                                        texts=nav['tokenized_text'],\n",
        "                                                        limit=limit_topic,\n",
        "                                                        start=start_topic,\n",
        "                                                        step=step_topic)\n",
        "\n",
        "\n",
        "# Find the optimal number of topics based on coherence score\n",
        "optimal_num_topics = start_topic + coherence_values.index(max(coherence_values))\n",
        "\n",
        "# Build LDA model with optimal number of topics\n",
        "lda_model = LdaMulticore(corpus=corpus, id2word=id2word, num_topics=optimal_num_topics, workers=1)  # Set workers to 1\n",
        "\n",
        "# Print the topics\n",
        "topics = lda_model.print_topics()\n",
        "for topic in topics:\n",
        "    print(topic)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66kNeGBJwGNO",
        "outputId": "d665f5c7-d0ba-4336-8f1f-656714c6a0c6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['ID', 'Name', 'Birthday', 'Bio', 'Cleaned_Bio'], dtype='object')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, '0.014*\"george\" + 0.007*\"watanabe\" + 0.006*\"mary\" + 0.005*\"frank\" + 0.005*\"murakami\" + 0.004*\"kay\" + 0.004*\"sato\" + 0.004*\"roy\" + 0.004*\"john\" + 0.004*\"bill\"')\n",
            "(1, '0.007*\"mary\" + 0.006*\"yamamoto\" + 0.005*\"frank\" + 0.004*\"nakano\" + 0.004*\"bill\" + 0.004*\"george\" + 0.004*\"kenji\" + 0.004*\"james\" + 0.003*\"jim\" + 0.003*\"ito\"')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "\n",
        "**Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1feafd0-d431-4cb3-954e-9024d0a05c4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['ID', 'Name', 'Birthday', 'Bio', 'Cleaned_Bio', 'processed_text',\n",
            "       'tokenized_text', 'Text (Clean)'],\n",
            "      dtype='object')\n",
            "Coherence score with 2 clusters: 0.7218997496386413\n",
            "Coherence score with 3 clusters: 0.7253380359053887\n",
            "Coherence score with 4 clusters: 0.6950874964484917\n",
            "Coherence score with 5 clusters: 0.7169365898272845\n",
            "Coherence score with 6 clusters: 0.6910716793598647\n",
            "Coherence score with 7 clusters: 0.6786220812614864\n",
            "Coherence score with 8 clusters: 0.7285658202914547\n",
            "Coherence score with 9 clusters: 0.7174757084604688\n",
            "Coherence score with 10 clusters: 0.677427421188213\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "import pandas as pd\n",
        "from gensim.models import LsiModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation, strip_short, stem_text, preprocess_string\n",
        "from gensim import corpora\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "# Assuming nav is your DataFrame containing Name_Info column\n",
        "\n",
        "# Preprocess text data\n",
        "def preprocess(text):\n",
        "    CUSTOM_FILTERS = [lambda x: x.lower(),\n",
        "                      remove_stopwords,\n",
        "                      strip_punctuation,\n",
        "                      strip_short,\n",
        "                      stem_text]\n",
        "    text = preprocess_string(text, CUSTOM_FILTERS)\n",
        "    return text\n",
        "\n",
        "print(nav.columns)  # Corrected typo\n",
        "# Preprocess text data\n",
        "nav['Text (Clean)'] = nav['Name'].apply(lambda x: preprocess(x))  # Corrected DataFrame name\n",
        "\n",
        "# Create a dictionary with the corpus\n",
        "dictionary = corpora.Dictionary(nav['Text (Clean)'])  # Corrected variable name\n",
        "\n",
        "# Convert corpus into a bag of words\n",
        "bow = [dictionary.doc2bow(text) for text in nav['Text (Clean)']]  # Corrected variable name\n",
        "\n",
        "# Find the optimal number of topics using coherence score\n",
        "coherence_values = []\n",
        "for num_topics in range(2, 11):\n",
        "    lsi = LsiModel(bow, num_topics=num_topics, id2word=dictionary)\n",
        "    coherence_model = CoherenceModel(model=lsi, texts=nav['Text (Clean)'], dictionary=dictionary, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    coherence_values.append((num_topics, coherence_score))\n",
        "    print('Coherence score with {} clusters: {}'.format(num_topics, coherence_score))\n",
        "\n",
        "# Choose the optimal number of topics based on coherence score\n",
        "optimal_num_topics = max(coherence_values, key=lambda x: x[1])[0]\n",
        "\n",
        "# Perform LSA to generate K topics\n",
        "lsi = LsiModel(bow, num_topics=optimal_num_topics, id2word=dictionary)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the top words in each topic\n",
        "for topic_num, words in lsi.print_topics(num_words=10):  # Updated variable name in the loop\n",
        "    print('Words in topic {}: {}.'.format(topic_num, words))\n",
        "\n",
        "# Find the scores given between the review and each topic\n",
        "corpus_lsi = lsi[bow]\n",
        "topic_scores = []\n",
        "for doc in corpus_lsi:\n",
        "    topic_scores.append([round(val[1], 2) for val in doc])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEcrqUBtxgS-",
        "outputId": "f4754c24-39db-45ea-c597-46c07401b9f2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words in topic 0: -0.978*\"georg\" + -0.048*\"murakami\" + -0.046*\"joe\" + -0.044*\"nakano\" + -0.041*\"kiyo\" + -0.040*\"yamada\" + -0.040*\"yoshida\" + -0.039*\"matsumoto\" + -0.038*\"maeda\" + -0.038*\"iseri\".\n",
            "Words in topic 1: 0.951*\"mari\" + 0.145*\"watanab\" + 0.059*\"jane\" + 0.056*\"suzuki\" + 0.056*\"nakamura\" + 0.054*\"okazaki\" + 0.054*\"ikeda\" + 0.053*\"yoshida\" + 0.053*\"nakata\" + 0.053*\"tomita\".\n",
            "Words in topic 2: 0.956*\"frank\" + 0.124*\"sato\" + 0.075*\"emi\" + 0.074*\"fujii\" + 0.071*\"fukuhara\" + 0.064*\"sumida\" + 0.063*\"konishi\" + 0.061*\"sata\" + 0.060*\"kikuchi\" + 0.060*\"yamasaki\".\n",
            "Words in topic 3: 0.940*\"watanab\" + -0.141*\"mari\" + 0.124*\"grace\" + 0.080*\"ann\" + 0.080*\"june\" + 0.078*\"kimura\" + 0.069*\"martha\" + 0.068*\"yuki\" + 0.067*\"amatatsu\" + 0.066*\"iku\".\n",
            "Words in topic 4: 0.930*\"yamamoto\" + 0.196*\"richard\" + 0.113*\"murakami\" + 0.088*\"sumiko\" + 0.084*\"emi\" + 0.080*\"kazu\" + 0.079*\"yuriko\" + 0.078*\"kaz\" + 0.070*\"madelon\" + 0.070*\"arai\".\n",
            "Words in topic 5: 0.751*\"murakami\" + 0.483*\"richard\" + 0.219*\"roi\" + -0.193*\"yamamoto\" + 0.159*\"masako\" + 0.091*\"judi\" + 0.083*\"tanimoto\" + 0.082*\"ayako\" + 0.070*\"hagiwara\" + 0.070*\"haruy\".\n",
            "Words in topic 6: 0.842*\"john\" + 0.425*\"jack\" + 0.126*\"herzig\" + 0.114*\"svahn\" + 0.101*\"tomita\" + 0.091*\"kat\" + 0.083*\"marumoto\" + 0.076*\"kanda\" + 0.076*\"hayakawa\" + 0.076*\"tateishi\".\n",
            "Words in topic 7: -0.768*\"jame\" + -0.392*\"nakano\" + -0.266*\"ito\" + -0.176*\"henri\" + -0.153*\"suzuki\" + -0.153*\"sakamoto\" + -0.120*\"nishimura\" + -0.083*\"turk\" + -0.079*\"yamazaki\" + -0.077*\"johnston\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame to show scores assigned for each topic for each review\n",
        "topic_df = pd.DataFrame(topic_scores, columns=['Topic {}'.format(i) for i in range(optimal_num_topics)])  # Updated variable name\n",
        "print(topic_df)  # Corrected line\n",
        "topic_df['Text'] = nav['Text (Clean)']  # Corrected DataFrame name\n",
        "topic_df['Dominant Topic'] = topic_df.iloc[:, :optimal_num_topics].idxmax(axis=1)\n",
        "\n",
        "# Find a sample review from each topic\n",
        "for i in range(optimal_num_topics):\n",
        "    topic_df_i = topic_df[topic_df['Dominant Topic'] == 'Topic {}'.format(i)]  # Updated variable name\n",
        "    if not topic_df_i.empty:\n",
        "        sample_text = topic_df_i.sample(1, random_state=2)['Text'].values[0]\n",
        "        print('Sample text from topic {}:\\n{}'.format(i, sample_text))\n",
        "    else:\n",
        "        print('No sample text available for topic {}'.format(i))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OehzuoYxhqo",
        "outputId": "10aeb22e-3aa8-4e4c-e730-a1a8b8a46197"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Topic 0  Topic 1  Topic 2  Topic 3  Topic 4  Topic 5  Topic 6  Topic 7\n",
            "0       -0.0      0.0    -0.01     0.01     0.01    -0.01     0.02    -0.06\n",
            "1        0.0     -0.0     0.01    -0.00     0.01    -0.00     0.00     0.00\n",
            "2       -0.0      0.0    -0.00     0.00    -0.00    -0.02    -0.01    -0.01\n",
            "3        0.0      0.0     0.00    -0.01    -0.01     0.02     0.00     0.00\n",
            "4        0.0     -0.0    -0.00    -0.00     0.00     0.00     0.00    -0.00\n",
            "..       ...      ...      ...      ...      ...      ...      ...      ...\n",
            "972      0.0      0.0    -0.00    -0.00     0.00    -0.00     0.00    -0.00\n",
            "973     -0.0      0.0     0.00    -0.00     0.00     0.02     0.92     0.01\n",
            "974     -0.0     -0.0     0.00     0.00    -0.00    -0.01    -0.01    -0.01\n",
            "975      0.0      0.0    -0.00     0.07    -0.00     0.01    -0.01    -0.01\n",
            "976      0.0      0.0    -0.00    -0.00     0.00     0.01    -0.00     0.00\n",
            "\n",
            "[977 rows x 8 columns]\n",
            "Sample text from topic 0:\n",
            "['toyoko', 'okumura']\n",
            "Sample text from topic 1:\n",
            "['mari', 'hirata']\n",
            "Sample text from topic 2:\n",
            "['susi', 'jinx', 'fujii']\n",
            "Sample text from topic 3:\n",
            "['yoshino', 'grace', 'fukuhara', 'niwa']\n",
            "Sample text from topic 4:\n",
            "['emlei', 'emi', 'kuboyama']\n",
            "Sample text from topic 5:\n",
            "['rubi', 'inouy']\n",
            "Sample text from topic 6:\n",
            "['gu', 'solomon']\n",
            "Sample text from topic 7:\n",
            "['georg', 'kikuta']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "**Generate K topics by using lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bertopic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZJjFan7yA8k",
        "outputId": "5cb268fa-b47d-47bc-fb28-bf5b4a513122"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bertopic\n",
            "  Downloading bertopic-0.16.0-py2.py3-none-any.whl (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m153.6/154.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.1/154.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.25.2)\n",
            "Collecting hdbscan>=0.8.29 (from bertopic)\n",
            "  Downloading hdbscan-0.8.33.tar.gz (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting umap-learn>=0.5.0 (from bertopic)\n",
            "  Downloading umap-learn-0.5.5.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.9/90.9 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.2.2)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (4.66.2)\n",
            "Collecting sentence-transformers>=0.4.1 (from bertopic)\n",
            "  Downloading sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.3/163.3 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (5.15.0)\n",
            "Collecting cython<3,>=0.27 (from hdbscan>=0.8.29->bertopic)\n",
            "  Using cached Cython-0.29.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2023.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (24.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.4.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.38.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (9.4.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.58.1)\n",
            "Collecting pynndescent>=0.5 (from umap-learn>=0.5.0->bertopic)\n",
            "  Downloading pynndescent-0.5.12-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (4.10.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.41.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->bertopic) (1.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->bertopic) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->bertopic) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers>=0.4.1->bertopic) (0.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
            "Building wheels for collected packages: hdbscan, umap-learn\n",
            "  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.33-cp310-cp310-linux_x86_64.whl size=3039293 sha256=5955f09d90fa0e31770bddf4610dc96cb697cfa5fbb12da5b2a1e478d24eeecb\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/0b/3b/dc4f60b7cc455efaefb62883a7483e76f09d06ca81cf87d610\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.5-py3-none-any.whl size=86832 sha256=887878530d182faeec46afb448ba3706a39ad45e58f970caf88df7e244c7647a\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/70/07/428d2b58660a1a3b431db59b806a10da736612ebbc66c1bcc5\n",
            "Successfully built hdbscan umap-learn\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, cython, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pynndescent, nvidia-cusolver-cu12, hdbscan, umap-learn, sentence-transformers, bertopic\n",
            "  Attempting uninstall: cython\n",
            "    Found existing installation: Cython 3.0.9\n",
            "    Uninstalling Cython-3.0.9:\n",
            "      Successfully uninstalled Cython-3.0.9\n",
            "Successfully installed bertopic-0.16.0 cython-0.29.37 hdbscan-0.8.33 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 pynndescent-0.5.12 sentence-transformers-2.6.1 umap-learn-0.5.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335,
          "referenced_widgets": [
            "0be165622b5f4ee6a6ef83b529b99a9a",
            "f1c5ac38a55e4ee5858d78f6be4bd3f1",
            "2b423d35908b4ec58a90c66820cab386",
            "0ccfe99598704a76b89bef11e760f707",
            "0a92df55679540d19fdcdd6610024f81",
            "cf9a7f2c04874ce2b008e9df60fb7dad",
            "6ef1016fdaf24865988e81eaffd7c0c9",
            "3b1212fcf1234165b2749d8a3e4ffe8e",
            "6c36272f82874bec9768923f6076ef27",
            "2560e401fbfc489bbd1e034074c989d1",
            "53c9441a885041a49d7af178b3a0fab1"
          ]
        },
        "outputId": "a99257aa-c9bc-4adf-9580-017aeaadb44f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-30 01:19:13,857 - BERTopic - Embedding - Transforming documents to embeddings.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/31 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0be165622b5f4ee6a6ef83b529b99a9a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-30 01:19:19,581 - BERTopic - Embedding - Completed ✓\n",
            "2024-03-30 01:19:19,584 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
            "2024-03-30 01:19:23,927 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-03-30 01:19:23,930 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
            "2024-03-30 01:19:23,999 - BERTopic - Cluster - Completed ✓\n",
            "2024-03-30 01:19:24,006 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
            "2024-03-30 01:19:24,024 - BERTopic - Representation - Completed ✓\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence Score: 0.5195231332289313\n",
            "Number of Topics (K): 2\n",
            "\n",
            "Topic Summaries:\n",
            "Topic 0: george, mary, frank, watanabe, bill, murakami, john, james, sato, kay\n",
            "\n",
            "Topic 1: yamamoto, eric, hisaye, arai, kojima, madelon, fusako, takenori, tats, yoneo\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "import pandas as pd\n",
        "from bertopic import BERTopic\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "# Read the CSV file\n",
        "data = pd.read_csv(\"/root/cleaned_data.csv\")  # Changed variable name to 'data'\n",
        "\n",
        "# Initialize BERTopic model\n",
        "topic_model = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True)\n",
        "\n",
        "# Convert values in 'Name_Info' column to strings\n",
        "data['Name_Info'] = data['Name'].astype(str)  # Updated column name\n",
        "\n",
        "# Fit BERTopic model on the preprocessed text data\n",
        "topics, probabilities = topic_model.fit_transform(data['Name_Info'])\n",
        "\n",
        "# Get top words per topic\n",
        "topics_info = topic_model.get_topics()\n",
        "top_words_per_topic = [[word for word, _ in words] for topic_id, words in topics_info.items() if topic_id != -1]\n",
        "\n",
        "# Tokenize and preprocess documents\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "processed_docs = [[word for word in tokenizer.tokenize(doc.lower()) if word not in stop_words] for doc in data['Name_Info'].astype(str).tolist()]\n",
        "\n",
        "# Create dictionary and corpus for coherence calculation\n",
        "dictionary = Dictionary(processed_docs)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "\n",
        "# Calculate coherence score\n",
        "coherence_model = CoherenceModel(topics=top_words_per_topic, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
        "coherence_score = coherence_model.get_coherence()\n",
        "print(f\"Coherence Score: {coherence_score}\")\n",
        "\n",
        "# Determine the number of topics\n",
        "num_topics = len(topics_info) - 1 if -1 in topics_info else len(topics_info)\n",
        "print(f\"Number of Topics (K): {num_topics}\")\n",
        "\n",
        "# Summarize Topics\n",
        "print(\"\\nTopic Summaries:\")\n",
        "for topic_id, words in topics_info.items():\n",
        "    if topic_id != -1:\n",
        "        topic_summary = \", \".join([word for word, _ in words])\n",
        "        print(f\"Topic {topic_id}: {topic_summary}\\n\")\n",
        "    else:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "**Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics.**\n",
        "\n",
        "You may refer the code here: https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335,
          "referenced_widgets": [
            "73074b41faa14790bb55d9ae2b449a10",
            "32d2090b2e1945a5a35198a18bcd1221",
            "f440beaa6ad34bc88f7dada0c486db4f",
            "c236dcc526b246f0bccd600084bfe6fe",
            "f370010401cd44f789b6e7d6a3fe9d09",
            "c929669323c84169baf8547d3a30989e",
            "1f97995ed0b74b9a93f4a638f74808d8",
            "bf024a6e55e74a1691e957e95f39d5c6",
            "6cb3771379314d268d45815174c41e37",
            "582548fdd00740d7a76e36b9bb3eb18b",
            "53cc8726ce9e43b8b660ea1e408cf638"
          ]
        },
        "outputId": "32298259-8494-4967-8445-f23a19d7c60c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-30 01:28:29,432 - BERTopic - Embedding - Transforming documents to embeddings.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/31 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "73074b41faa14790bb55d9ae2b449a10"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-30 01:28:34,508 - BERTopic - Embedding - Completed ✓\n",
            "2024-03-30 01:28:34,511 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
            "2024-03-30 01:28:38,918 - BERTopic - Dimensionality - Completed ✓\n",
            "2024-03-30 01:28:38,920 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
            "2024-03-30 01:28:38,990 - BERTopic - Cluster - Completed ✓\n",
            "2024-03-30 01:28:38,996 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
            "2024-03-30 01:28:39,014 - BERTopic - Representation - Completed ✓\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence Score: 0.5188527786198915\n",
            "Number of Topics (K): 2\n",
            "\n",
            "Topic Summaries:\n",
            "Topic 0: george, mary, frank, watanabe, bill, murakami, john, james, sato, kay\n",
            "\n",
            "Topic 1: yamamoto, eric, hisaye, arai, kojima, madelon, fusako, takenori, tats, yoneo\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "from bertopic import BERTopic\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "# Initialize BERTopic model\n",
        "topic_model = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True)\n",
        "\n",
        "# Assuming nav is your DataFrame containing the data\n",
        "# Tokenize and preprocess documents\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Fit BERTopic model on the preprocessed text data\n",
        "topics, probabilities = topic_model.fit_transform(nav['processed_text'].tolist())  # Assuming 'processed_text' contains preprocessed text data\n",
        "\n",
        "# Get top words per topic\n",
        "topics_info = topic_model.get_topics()\n",
        "top_words_per_topic = [[word for word, _ in words] for topic_id, words in topics_info.items() if topic_id != -1]\n",
        "\n",
        "# Tokenize each document separately\n",
        "tokenized_docs = [doc.split() for doc in nav['processed_text']]\n",
        "# Create dictionary and corpus for coherence calculation\n",
        "dictionary = Dictionary(tokenized_docs)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "\n",
        "# Calculate coherence score\n",
        "coherence_model = CoherenceModel(topics=top_words_per_topic, texts=tokenized_docs, dictionary=dictionary, coherence='c_v')\n",
        "coherence_score = coherence_model.get_coherence()\n",
        "print(f\"Coherence Score: {coherence_score}\")\n",
        "\n",
        "# Determine the number of topics\n",
        "num_topics = len(topics_info) - 1 if -1 in topics_info else len(topics_info)\n",
        "print(f\"Number of Topics (K): {num_topics}\")\n",
        "\n",
        "# Summarize Topics\n",
        "print(\"\\nTopic Summaries:\")\n",
        "for topic_id, words in topics_info.items():\n",
        "    if topic_id != -1:\n",
        "        topic_summary = \", \".join([word for word, _ in words])\n",
        "        print(f\"Topic {topic_id}: {topic_summary}\\n\")\n",
        "    else:\n",
        "        break  # No more topics to print\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra Question (5 Points)\n",
        "\n",
        "**Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details.**\n",
        "\n",
        "**This question will compensate for any points deducted in this exercise. Maximum marks for the exercise is 40 points.**"
      ],
      "metadata": {
        "id": "d89ODUx3jjJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "'''\n",
        "LDA:The result shows terms and their probabilities, showing whether or not they are associated with sentiment analysis\n",
        "    positively or negatively. positive associations are generally associated with pleasant emotions like contentment\n",
        "    or happiness, whereas negative associations are associated with negative emotions like sadness or discontent.\n",
        "LSA:takes sample text from each topic and uses coherence scores to determine if the material expresses a favorable\n",
        "     or negative sentiment. These ratings offer a measure of the issues' coherence or consistency, which may be a proxy for\n",
        "     the emotions expressed in them.\n",
        "lda2vec and BERTopic:Each topic is summarized in the output by emphasizing important terms related to the subject.\n",
        "                     This succinct synopsis makes it easier to comprehend the key ideas contained in each topic, which\n",
        "                     facilitates interpretation and analysis.\n",
        "Generally, LDA and LSA perform better than BERTopic and lda2vec. In situations where clear insights are crucial,\n",
        "LDA in particular is preferred due to its simplicity and interpretability. On the other hand, even if they may execute more slowly\n",
        "or with greater complexity, lda2vec and BERTopic are better suited to extract more complicated associations from the data.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "OK34nZtojhmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "Consider the following points in your response:\n",
        "\n",
        "**Learning Experience:** Describe your overall learning experience in working with text data and extracting features using various topic modeling algorithms. Did you understand these algorithms and did the implementations helped in grasping the nuances of feature extraction from text data.\n",
        "\n",
        "**Challenges Encountered:** Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "It gave exposure to various topic modeling algorithms such as Latent Dirichlet Allocation (LDA), Latent Semantic Analysis (LSA), lda2vec, and BERTopic.\n",
        "and understanding their differences helps in choosing the appropriate one for different tasks. It was my first time but a fun valuable learning. the challenge i faced in 3rd question was i was unable to\n",
        "install the lda2vec in the google colab even after multiple attempt so then i continued with BERTopic everything else went well took some time for errors but able to execute with the help of online tools.\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0be165622b5f4ee6a6ef83b529b99a9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f1c5ac38a55e4ee5858d78f6be4bd3f1",
              "IPY_MODEL_2b423d35908b4ec58a90c66820cab386",
              "IPY_MODEL_0ccfe99598704a76b89bef11e760f707"
            ],
            "layout": "IPY_MODEL_0a92df55679540d19fdcdd6610024f81"
          }
        },
        "f1c5ac38a55e4ee5858d78f6be4bd3f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf9a7f2c04874ce2b008e9df60fb7dad",
            "placeholder": "​",
            "style": "IPY_MODEL_6ef1016fdaf24865988e81eaffd7c0c9",
            "value": "Batches: 100%"
          }
        },
        "2b423d35908b4ec58a90c66820cab386": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b1212fcf1234165b2749d8a3e4ffe8e",
            "max": 31,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c36272f82874bec9768923f6076ef27",
            "value": 31
          }
        },
        "0ccfe99598704a76b89bef11e760f707": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2560e401fbfc489bbd1e034074c989d1",
            "placeholder": "​",
            "style": "IPY_MODEL_53c9441a885041a49d7af178b3a0fab1",
            "value": " 31/31 [00:04&lt;00:00, 11.08it/s]"
          }
        },
        "0a92df55679540d19fdcdd6610024f81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf9a7f2c04874ce2b008e9df60fb7dad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ef1016fdaf24865988e81eaffd7c0c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b1212fcf1234165b2749d8a3e4ffe8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c36272f82874bec9768923f6076ef27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2560e401fbfc489bbd1e034074c989d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53c9441a885041a49d7af178b3a0fab1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73074b41faa14790bb55d9ae2b449a10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_32d2090b2e1945a5a35198a18bcd1221",
              "IPY_MODEL_f440beaa6ad34bc88f7dada0c486db4f",
              "IPY_MODEL_c236dcc526b246f0bccd600084bfe6fe"
            ],
            "layout": "IPY_MODEL_f370010401cd44f789b6e7d6a3fe9d09"
          }
        },
        "32d2090b2e1945a5a35198a18bcd1221": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c929669323c84169baf8547d3a30989e",
            "placeholder": "​",
            "style": "IPY_MODEL_1f97995ed0b74b9a93f4a638f74808d8",
            "value": "Batches: 100%"
          }
        },
        "f440beaa6ad34bc88f7dada0c486db4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf024a6e55e74a1691e957e95f39d5c6",
            "max": 31,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6cb3771379314d268d45815174c41e37",
            "value": 31
          }
        },
        "c236dcc526b246f0bccd600084bfe6fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_582548fdd00740d7a76e36b9bb3eb18b",
            "placeholder": "​",
            "style": "IPY_MODEL_53cc8726ce9e43b8b660ea1e408cf638",
            "value": " 31/31 [00:04&lt;00:00, 11.17it/s]"
          }
        },
        "f370010401cd44f789b6e7d6a3fe9d09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c929669323c84169baf8547d3a30989e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f97995ed0b74b9a93f4a638f74808d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf024a6e55e74a1691e957e95f39d5c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cb3771379314d268d45815174c41e37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "582548fdd00740d7a76e36b9bb3eb18b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53cc8726ce9e43b8b660ea1e408cf638": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}